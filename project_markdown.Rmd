---
title: "Predicting CO2 Emissions by Power Plant"
author: "Owen Jetton"
date: "3/10/2021"
output: github_document
---

# Data

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
 
# Download and load packages
library(pacman)

p_load(readr, dplyr, ggplot2, readxl, magrittr, janitor, utils, tidyverse, tidymodels,
       tune, glmnet, recipes, rpart, xgboost)
```

```{r reading data}
# Read CO2 data from 2016 - 19
co2_data_2016 = read_xlsx("C:/Users/ojett/Documents/EC424/ec424project/emissions2016.xlsx",
                     sheet = 1) %>% mutate(year = 2016)
                        # add year variable which will be helpful for merging data later

co2_data_2017 = read_xlsx("C:/Users/ojett/Documents/EC424/ec424project/emissions2017.xlsx",
                     sheet = 1) %>% mutate(year = 2017)

co2_data_2018 = read_xlsx("C:/Users/ojett/Documents/EC424/ec424project/emissions2018.xlsx",
                     sheet = 1) %>% mutate(year = 2018)

co2_data_2019 = read_xlsx("C:/Users/ojett/Documents/EC424/ec424project/emissions2019.xlsx",
                     sheet = 1) %>% mutate(year = 2019)


# Upload heating and cooling days data
heat_cool_df = read.delim("https://www.ncei.noaa.gov/pub/orders/CDODiv6010178275591.txt",
                          sep = ",") 


```

# Cleaning

```{r Cleaning Data}

# Merging CO2 to get larger data set
data_full = do.call("rbind", list(co2_data_2016, co2_data_2017,
                                  co2_data_2018, co2_data_2019))


# Cleaning the full CO2 data
data_full %<>% clean_names() %>%
  
    # only care about power plants that generate electricity
  filter(generation_k_wh > 0) %>% 
  
    # remove irrelevent or repretitive variables
  select(-c(eia_balancing_authority_region, balancing_authority_name, nerc_region, 
            fuel_units, metric_tonnes_of_co2_emissions, plant_name))



# Cleaning the Heating and Cooling days data
heat_cool_df %<>% clean_names() %>% 
    # select relevant variables
  select(state_code, year_month, cdd, hdd) %>%
  
    # make year variable from year_month variable, cutting off the month letters
  mutate(year = strtrim(year_month,4)) %>%
    
    # grouping by months, and taking the sum to get the total number
      # of heating and cooling days in a year
  group_by(state_code, year) %>%
  mutate(cooling_days = sum(cdd),
         heating_days = sum(hdd),
         state_code = factor(state_code)) %>%
  
    # selecting relevant 
  select(state_code, year, cooling_days, heating_days) %>%
  distinct()

# Need the state names to correspond with the right regions:
  # read dataset that has state names for corresponding regions
  states_region = read_xlsx("C:/Users/ojett/Documents/EC424/ec424project/region_to_state.xlsx")
  
  # add state names to respective region in 
heat_cool_df = merge(heat_cool_df, states_region, by = "state_code")


# Add heating and cooling data to main dataframe

data_full = merge(data_full, heat_cool_df, by = c("state", "year"))


# Renaming CO2 emissions variable to "emissions"
  # I'm doing this because it is an important variable and will need to be written a lot
data_full %<>% rename(emissions = tons_of_co2_emissions) %>%
  
  # If there is no balancing authority
  mutate(balancing_authority_code = if_else(is.na(balancing_authority_code), "NONE",
                                            balancing_authority_code)) %>%
  
  # Also turn certain numerical variables into factors
  mutate_at(.vars = vars("sector_code", "balancing_authority_code", "state_code"),
            .funs = factor) 



```



```{r Initial Graphs}
# Graphs Checking out the data
ggplot(data_full, aes(x = log(generation_k_wh), y = log(emissions))) +
  geom_point(aes(color = aggregated_fuel_group)) +
  labs(x = "Generation (KWH)", y = "Tons of CO2 Emissions",
       title = "Power Generated Against CO2 Emissions by Fuel Group (Logged Values)",
      color = "Fuel Group",
      caption = "PET = Petroleum, MSW = Municipal solid waste, GEO = Geothermal, GAS = Natural Gas, COAL = Coal") +
  theme_light()


ggplot(data_full, aes(y = aggregated_fuel_group, fill = aggregated_fuel_group)) + 
  geom_bar(stat = "count") +
  labs(x = "Total", y = "Fuel Group",
       title = "Frequency of Fuel Groups in the Data") +
  theme_light() +
  theme(legend.position = "none")

  # Note: Gas is ALL natural gas. MSW is Munincipal Solid Waste. PET is petroleum. 
          # Coal is mostly Bituminous Coal. PET is petroleum, mostly Distillate Fuel Oil
          # GEO is geothermal
  # full table explaining fuel codes here:
    # https://www.eia.gov/electricity/annual/html/epa_a_03.html

```

# Tuning and Training

### Split into Training and Testing Data
```{r training / testing split}
# Set seed before splitting
set.seed(231240)

# Initial 80/20 split for the training/testing data
data_split = data_full %>% initial_split(prop = 0.8)

  # Create training data
train_df = data_split %>% training()

  # Create testing data
test_df = data_split %>% testing()

```

### Create and Tune 4 models on Training data
```{r Creating and Tuning my models}
# ------------------------------ Set up --------------------------------
# Creating Recipe used for multiple models below
train_recipe = train_df %>% recipe(emissions ~ .) %>%
          
                      # update role
                      update_role(plant_code, new_role = "id variable") %>%
  
                      # Add dummy variables for all nominal variables
                      step_dummy(all_nominal()) %>%

                      # Mean imputation for numeric predictors
                      step_meanimpute(all_predictors() & all_numeric()) %>% 
  
                      # normalize
                      step_normalize(all_predictors() & all_numeric()) %>%
                      
                      # Remove low-variance
                      step_nzv(all_predictors() & all_numeric()) %>%
  
                      # Remove highly correlated
                      step_corr(all_predictors() & all_numeric()) %>%
  
                      # Remove linearly dependent predictors
                      step_lincomb(all_predictors() & all_numeric()) 


# setting new seed for CV folds
set.seed(1387219452)

  # Create Cross Validcation folds
train_cv = train_df %>% vfold_cv(v = 5)


```


#### Linear Regression Model
```{r Models}

# --------------------- Basic linear regression ----------------------

# Fitting the linear regression model to training data
reg_model = 
  linear_reg() %>%
  set_engine("lm")

# Create workflow
workflow_reg = workflow() %>%
  add_model(reg_model) %>%
  add_recipe(train_recipe)

# Train model on validation sets, set metric to root mean square error
cv_reg = workflow_reg %>% 
  fit_resamples(train_cv, metrics = metric_set(rmse))

# Finalize workflow to get our regression model!
final_reg = workflow_reg %>%
  finalize_workflow(select_best(cv_reg, 'rmse'))  %>%
  fit(data = train_df)

```

#### Elasticnet Model
```{r Elasticnet model}
# ------------------------ Elasticnet --------------------------------

# DEFINE ELASTICNET MODEL
en_model = linear_reg(penalty = tune(), mixture = tune()) %>% 
            set_engine("glmnet")

  # Define workflow
workflow_en = workflow() %>%
  add_model(en_model) %>%
  add_recipe(train_recipe) 

  # Define Penalties
lambdas = 10^seq(from = 5, to = -2, length = 1e2)
alphas = seq(from = 0, to = 1, by = 0.1)

  # Cross Validation Tuning with range of penalites and mixtures
cv_en = workflow_en %>%
  tune_grid(
    train_cv,
    grid = expand_grid(mixture = alphas, penalty = lambdas),
    metrics = metric_set(rmse)
  )

  # Finalize workflow for final model
final_en = workflow_en %>%
  finalize_workflow(select_best(cv_en, 'rmse'))  %>%
  fit(data = train_df)

# Wants penalty = 0.01, mixture = 0.3

```

#### Decision Tree Model
```{r decision tree model}
# ---------------------------- Decision Tree ------------------------------

# Creating regression tree
tree_model = decision_tree(
  mode = "regression",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5
) %>% set_engine("rpart")

# Define workflow
workflow_tree = workflow() %>%
  add_model(tree_model) %>%
  add_recipe(train_recipe)

# Tune with metrics root mean square error and r-squared
tree_cv_fit = workflow_tree %>%
  tune_grid(
    train_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 10, by = 0.5),
      tree_depth = seq(1, 10, by = 1)
    ),
    metrics = metric_set(rmse)
  )

# finalize workflow for plotting and predicting
best_flow = workflow_tree %>%
  finalize_workflow(select_best(tree_cv_fit, 
                                metric =  "rmse")) %>%
  fit(data = train_df)

# Extract fitted model

best_tree = best_flow %>% pull_workflow_fit()


```


#### Boosted Ensemble model
```{r Boosted model}
# ------------------------------ Boosting --------------------------------

# Creating the model - regression tree boosted emsemble model
boost_model = boost_tree(
  mtry = NULL,
  trees = 100,
  min_n = NULL,
  tree_depth = tune(),
  learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>%
  set_mode("regression")


# define workflow
workflow_boost = workflow() %>%
  add_model(boost_model) %>%
  add_recipe(train_recipe)

# Run the model
cv_boost = workflow_boost %>%
  tune_grid(
    train_cv,
    grid = grid_regular(tree_depth(),
                        learn_rate(),
                        levels = 5:5),
    metrics = metric_set(rmse)
    )

# Finalize workflow and fit to training data
final_boost = workflow_boost %>%
  finalize_workflow(select_best(cv_boost, "rmse")) %>% 
  fit(data = train_df)

```

### Estimate CV Error
```{r Estimated Error}
#  Estimate your error (with an appropriately chosen metric) using cross validation

# Linear Regression Root Mean Square Error
    # Averaging the CV RMSE errors from tuning
reg_error = sum(cv_reg[[3]][[1]]$.estimate + cv_reg[[3]][[2]]$.estimate +
                   cv_reg[[3]][[3]]$.estimate + cv_reg[[3]][[4]]$.estimate +
                   cv_reg[[3]][[5]]$.estimate)/5


# Elasticnet Root Mean Square Error
    # values extracted manually through cv_en dataset estimates where
    # penalty = 0.01, and mixture = 0.3
elas_error = sum(124365.9 + 123989.3 + 119269.7 + 67415.22 + 123227.2)/5


# Decision Tree Root Mean Square Error
    # values extracted from tree_cv_fit dataset with corre
tree_error = sum(1.303645e+05 + 8.239833e+04 + 5.612436e+04 + 9.301760e+04 +
                   1.359426e+05)/5


# Boosted Model Root Mean Square Error
    # values extracted manually from the cv_boost dataset with correct tree
    # depth (8) and complexity (0.1) that the final boosted model has
boost_error = sum(86721.87 + 41914.81 + 40989.52 + 46046.91 + 42405.41)/5



# Comparing Cross Validation Errors
cv_error_df = data.frame(Model = c("Linear Regression", "Elasticnet",
                                   "Decision Tree", "Boosted"),
                         Error = c(reg_error, elas_error, tree_error,
                                      boost_error))

ggplot(cv_error_df, aes(x = Model, y = log(Error))) +
  geom_point(shape = 2) +
  labs(x = "Model", y = "Logged CV RMSE",
       title = "Estimated Cross Validation Error By Model") +
  theme_light()

```

# Prediction

### Test on test data
```{r Prediction}
# Predicting with Linear Regression
reg_pred = final_reg %>% predict(new_data = test_df)

# Predicting with Elasticnet
en_pred = final_en %>% predict(new_data = test_df)

# Predicting with Decision Tree
tree_pred = best_flow %>% predict(new_data = test_df)

# Predicting with Boosted Model - WORKED
boost_pred = final_boost %>% predict(new_data = test_df)


# Gathering the RMSE's of the predictions into a dataframe. '
    # "rmse" function from Metrics package, but not until now b/c of conflict with
    # yardstick package
p_load(Metrics)

predict_rmse_df = data.frame(
  Model = c("Linear Regression", "Elasticnet", "Decision Tree", "Boosted"),
  Error = c(rmse(test_df$emissions, reg_pred$.pred),
           rmse(test_df$emissions, en_pred$.pred),
           rmse(test_df$emissions, tree_pred$.pred),
           rmse(test_df$emissions, boost_pred$.pred))
  )


# Plotting the RMSE of the predictions
ggplot(predict_rmse_df, aes(x = Model, y = log(Error))) +
  geom_point(shape) +
  labs(x = "Model", y = "Logged RMSE",
       title = "Out of Sample Error By Model") +
  theme_light()


# Putting the RMSE datasets together to compare
compare_df = rbind(cv_error_df, predict_rmse_df)

  # adding designation variable for graphing ease
compare_df %<>% mutate(Type = c("CV Error", "CV Error", "CV Error", "CV Error",
                                "Test Error", "Test Error", "Test Error", "Test Error"))


# Graphing a comparison
ggplot(compare_df, aes(x = Model, y = Error, shape = Type)) +
  geom_point() +
  labs(x = "Model", y = "Logged RMSE",
       title = "Comparing Estimated vs Real Error By Model") +
  theme_light() +
  theme(legend.position = "bottom", legend.title = element_blank())

```

### Turn in: 
  1. Link to this github page with (commented) analysis code and figures
  2. 1-page "executive summary"
  3. 4-slide summary following paragraphs 2–5 of the executive summary
  4. Evaluation of your group member's contribution (doesn't apply)

### Sources
 * Emissions by power plant for CO2, SO2, and NOx (2019)
    https://www.eia.gov/electricity/data/emissions/ 
 * Guide to data
    https://www.eia.gov/electricity/data/guide/pdf/guide.pdf
 * Cooling and heating days data
    https://www7.ncdc.noaa.gov/CDO/CDODivisionalSelect.jsp#


